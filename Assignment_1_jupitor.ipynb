{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbC3DN7kqO4W",
        "outputId": "742a3c85-8673-463a-c55f-e8229f2b67b9"
      },
      "outputs": [],
      "source": [
        "# pip install -U scikit-learn\n",
        "# pip install wandb -qU\n",
        "# pip uninstall wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wandb login 494428cc53b5c21da594f4fc75035d136c63a93c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.init(project=\"CS6910 - Assignment 1\", name=\"Question1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Define class names\n",
        "className_images = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "trainingData_length=len(train_labels)\n",
        "total_class=max(train_labels)+1\n",
        "\n",
        "# print(trainingData_length)\n",
        "# print(y)\n",
        "\n",
        "#plotting images with its labels\n",
        "plt.figure(figsize=(5, 5))\n",
        "for i in range(total_class):\n",
        "    #subplot - used so that we can have many images in one image\n",
        "    # 1st row,2nd col, 3rd index of curr\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    sample_array = np.where(train_labels == i)\n",
        "    label_index=sample_array[0][0]\n",
        "    plt.imshow(train_images[label_index], cmap='gray')\n",
        "    plt.title(className_images[i])\n",
        "    plt.axis('off')\n",
        "    wandb.log({\"Question1\": [wandb.Image(train_images[label_index], caption=className_images[i])]})\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MultiLayerPerceptron class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron():\n",
        "\n",
        "    def __init__(\n",
        "          self,\n",
        "          train_data,\n",
        "          train_labels,\n",
        "          val_data,\n",
        "          val_labels,\n",
        "          layer_sizes,\n",
        "          initialization,\n",
        "          loss_act_func,\n",
        "          batch_size=4,\n",
        "          epochs=1,\n",
        "          learning_rate=0.1,\n",
        "          weight_decay=0,\n",
        "          optimizer=\"sgd\",\n",
        "          momentum=0.5,\n",
        "          beta=0.5,\n",
        "          beta1=0.5,\n",
        "          beta2=0.5,\n",
        "          epsilon=0.000001\n",
        "          ):\n",
        "\n",
        "        self.parameters = {}\n",
        "        self.gradients = {}\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "        self.val_data=val_data\n",
        "        self.val_labels=val_labels\n",
        "        self.no_of_samples = len(train_data)\n",
        "        self.no_of_features = train_data.shape[1] #size of column of train_data\n",
        "        self.no_of_classes = 10 #size of column of train_labels\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.batch_size=batch_size\n",
        "        # self.act_func=act_func\n",
        "        self.epochs=epochs\n",
        "        self.learning_rate=learning_rate\n",
        "        self.weight_decay=weight_decay\n",
        "        self.optimizer=optimizer\n",
        "        self.loss_act_func=loss_act_func\n",
        "        self.initialization = initialization\n",
        "        self.momentum=momentum\n",
        "        self.beta=beta\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.epsilon=epsilon\n",
        "\n",
        "        self.A = {}\n",
        "        self.H = {}\n",
        "\n",
        "    def init_weights(self):\n",
        "      if self.initialization==\"random\":\n",
        "        # print(\"init random\")\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            #input layer(n) and 1st hidden layer(m) will have weight vector w1->n*m and bias b1->1*m\n",
        "            #init W to random values\n",
        "            self.parameters[f'w_{i+1}'] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1])\n",
        "            #init B to 0\n",
        "            self.parameters[f'b_{i+1}'] = np.random.randn(1,self.layer_sizes[i+1])\n",
        "\n",
        "      if self.initialization==\"Xavier\":\n",
        "        print(\"inint Xavier\")\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "          x_fact = np.sqrt(6 / (self.layer_sizes[i] + self.layer_sizes[i + 1]))\n",
        "          self.parameters[f'w_{i+1}'] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * x_fact\n",
        "          self.parameters[f'b_{i+1}'] = np.random.randn(1,self.layer_sizes[i+1]) * x_fact\n",
        "\n",
        "    def softmax(self, x):\n",
        "      max_x = np.max(x, axis=1, keepdims=True)\n",
        "      exps = np.exp(x - max_x)\n",
        "      return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_d(self,x):\n",
        "      softmax_x = self.softmax(x)\n",
        "      return softmax_x*(1-softmax_x)\n",
        "\n",
        "    def feed_forwards(self, input_data):\n",
        "\n",
        "        # self.H[0]=input_data\n",
        "        for i in range(len(self.layer_sizes) - 1): # i -> 0-3\n",
        "            # if(i==0) :#input is our images\n",
        "            if(i==0):\n",
        "              preactivation=np.dot(input_data , self.parameters[f'w_{i+1}']) + self.parameters[f'b_{i+1}']\n",
        "            else:\n",
        "              preactivation = np.dot(self.H[i], self.parameters[f'w_{i+1}']) + self.parameters[f'b_{i+1}']\n",
        "            self.A[i+1]=preactivation\n",
        "\n",
        "            if i < len(self.layer_sizes) - 2 : # i < 3\n",
        "                activation = self.loss_act_func.activation_func(self.A[i+1])\n",
        "            else :\n",
        "                activation=self.softmax(self.A[i+1])\n",
        "            self.H[i+1]=activation\n",
        "\n",
        "        return self.H[len(self.layer_sizes)-1]\n",
        "\n",
        "\n",
        "    def back_prop(self, input_data, true_label,pred_label):\n",
        "        # print(\"in back\")\n",
        "        if ((self.loss_act_func).get_name()==\"cross_entropy\"):\n",
        "          # print(\"cross_entropy\")\n",
        "          dL_dA = pred_label - true_label\n",
        "        elif ((self.loss_act_func).get_name()==\"mean_squared_error\"):\n",
        "          dL_dA = ((pred_label - true_label) * self.softmax_d(self.A[len(self.layer_sizes)-1])) / true_label.shape[0]\n",
        "\n",
        "        for i in range(len(self.layer_sizes) - 1, 1, -1): # i->4 3 2\n",
        "\n",
        "            self.gradients[f'w_{i}']=np.dot(self.H[i-1].T, dL_dA)\n",
        "            self.gradients[f'b_{i}']=np.sum(dL_dA , axis=0,keepdims=True)\n",
        "\n",
        "            dL_dH=np.dot(dL_dA, (self.parameters[f'w_{i}']).T )\n",
        "            dL_dA = dL_dH * self.loss_act_func.derivation_of_activation(self.A[i-1])\n",
        "\n",
        "        self.gradients[f'w_{1}']=np.dot(input_data.T, dL_dA )\n",
        "        self.gradients[f'b_{1}']=np.sum(dL_dA , axis=0,keepdims=True)\n",
        "\n",
        "    def optimizer_func(self):\n",
        "\n",
        "        if(self.optimizer==\"sgd\"):\n",
        "            #this is batch gradient descent or SGD\n",
        "            # no_of_batches = self.no_of_samples // batch_size\n",
        "            for i in range(self.epochs):\n",
        "                for j in range(0,self.no_of_samples,self.batch_size):\n",
        "                    start = j\n",
        "                    end = start + self.batch_size\n",
        "                    Y_train_labels = self.train_labels[start:end]\n",
        "                    X_train_images = self.train_data[start:end]\n",
        "\n",
        "                    pred_labels=self.feed_forwards(X_train_images)\n",
        "                    # pred_labels=self.fprg(X_train_images)\n",
        "                    self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "\n",
        "                    for k in range(len(self.layer_sizes) - 1):# 0->3\n",
        "                        self.gradients[f'w_{k+1}']=self.gradients[f'w_{k+1}']/self.batch_size\n",
        "                        self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                        self.gradients[f'w_{k+1}'] =(self.gradients[f'w_{k+1}']) + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "                        self.parameters[f'w_{k+1}'] -= self.learning_rate * (self.gradients[f'w_{k+1}'] )\n",
        "                        self.parameters[f'b_{k+1}'] -= self.learning_rate * (self.gradients[f'b_{k+1}'] )\n",
        "\n",
        "                pred_labels = self.feed_forwards(self.train_data)\n",
        "                tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "                tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "                pred_labels = self.feed_forwards(self.val_data)\n",
        "                val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "                val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "                print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "                wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "        if(self.optimizer==\"momentum\"):\n",
        "          '''\n",
        "          Epoch 10, Loss: 0.6812374987038496 Acc: 75.0\n",
        "          TEST LOSS: 0.7075 ACCURACY: 72.0900\n",
        "          '''\n",
        "          velocities={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "            velocities[f'w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            velocities[f'b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          for i in range(self.epochs):\n",
        "            for j in range(0,self.no_of_samples,self.batch_size):\n",
        "              start = j\n",
        "              end = start + self.batch_size\n",
        "              Y_train_labels = self.train_labels[start:end]\n",
        "              X_train_images = self.train_data[start:end]\n",
        "\n",
        "              pred_labels=self.feed_forwards(X_train_images)\n",
        "              self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "\n",
        "              for k in range(len(self.layer_sizes) - 1):\n",
        "                  self.gradients[f'w_{k+1}']=self.gradients[f'w_{k+1}']/self.batch_size\n",
        "                  self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                  self.gradients[f'w_{k+1}'] =(self.gradients[f'w_{k+1}']) + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "                  # Update velocities with momentum\n",
        "                  velocities[f'w_{k+1}'] = self.momentum * velocities[f'w_{k+1}'] + self.learning_rate * (self.gradients[f'w_{k+1}'])\n",
        "                  velocities[f'b_{k+1}'] = self.momentum * velocities[f'b_{k+1}'] + self.learning_rate * (self.gradients[f'b_{k+1}'])\n",
        "\n",
        "                  # Update parameters with momentum\n",
        "                  self.parameters[f'w_{k+1}'] -= velocities[f'w_{k+1}']\n",
        "                  self.parameters[f'b_{k+1}'] -= velocities[f'b_{k+1}']\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.train_data)\n",
        "            tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "            tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.val_data)\n",
        "            val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "            val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "            print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "            wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "        if(self.optimizer==\"nag\"):\n",
        "            '''\n",
        "            Epoch 100, Loss: 0.8561739933889985 Acc: 63.355\n",
        "            TEST LOSS: 0.8864 ACCURACY: 62.7400\n",
        "\n",
        "            Epoch 20, Loss: 1.013375573148102 Acc: 61.795\n",
        "            TEST LOSS: 1.0499 ACCURACY: 60.2300\n",
        "            '''\n",
        "           # Initialize velocities for weights and biases\n",
        "            velocities={}\n",
        "            for i in range(len(self.layer_sizes) - 1):\n",
        "                velocities[f'w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "                velocities[f'b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "            for i in range(self.epochs):\n",
        "                for j in range(0,self.no_of_samples,self.batch_size):\n",
        "                    start = j\n",
        "                    end = start + self.batch_size\n",
        "                    Y_train_labels = self.train_labels[start:end]\n",
        "                    X_train_images = self.train_data[start:end]\n",
        "\n",
        "                    '''\n",
        "                    storing old weights in self.parameters[f'w_{k+1}_orig']\n",
        "                    so that i can use it later and\n",
        "                    updating self.parameters[f'w_{k+1}'] so that same feed forward function can be used.\n",
        "                    '''\n",
        "                    for k in range(len(self.layer_sizes) - 1):\n",
        "                        self.parameters[f'w_{k+1}_orig']=self.parameters[f'w_{k+1}']\n",
        "                        self.parameters[f'b_{k+1}_orig']=self.parameters[f'b_{k+1}']\n",
        "\n",
        "                        self.parameters[f'w_{k+1}'] = self.parameters[f'w_{k+1}'] - self.momentum * velocities[f'w_{k+1}']\n",
        "                        self.parameters[f'b_{k+1}'] = self.parameters[f'b_{k+1}'] - self.momentum * velocities[f'b_{k+1}']\n",
        "\n",
        "                    #feed forward and backprop is applied after changing weights and biases\n",
        "                    pred_labels=self.feed_forwards(X_train_images)\n",
        "                    self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "\n",
        "                    '''\n",
        "                    here 1st we will set original weights to self.parameters[f'w_{k+1}']\n",
        "                    and then update weigths and biases.\n",
        "                    '''\n",
        "                    for k in range(len(self.layer_sizes) - 1):\n",
        "\n",
        "                        self.gradients[f'w_{k+1}']=self.gradients[f'w_{k+1}']/self.batch_size\n",
        "                        self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                        self.gradients[f'w_{k+1}'] =(self.gradients[f'w_{k+1}']) + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "\n",
        "                        velocities[f'w_{k+1}'] = self.momentum * velocities[f'w_{k+1}'] + self.learning_rate * (self.gradients[f'w_{k+1}'] )\n",
        "                        velocities[f'b_{k+1}'] = self.momentum * velocities[f'b_{k+1}'] + self.learning_rate * (self.gradients[f'b_{k+1}'] )\n",
        "\n",
        "                        self.parameters[f'w_{k+1}']=self.parameters[f'w_{k+1}_orig']\n",
        "                        self.parameters[f'b_{k+1}']=self.parameters[f'b_{k+1}_orig']\n",
        "\n",
        "                        self.parameters[f'w_{k+1}'] -= velocities[f'w_{k+1}']\n",
        "                        self.parameters[f'b_{k+1}'] -= velocities[f'b_{k+1}']\n",
        "\n",
        "                pred_labels = self.feed_forwards(self.train_data)\n",
        "                tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "                tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "                pred_labels = self.feed_forwards(self.val_data)\n",
        "                val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "                val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "                print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "                wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "        if(self.optimizer==\"rmsprop\"):\n",
        "          '''\n",
        "          Epoch 10, Loss: 0.6455246819185498 Acc: 75.41499999999999\n",
        "          TEST LOSS: 0.6710 ACCURACY: 74.4000\n",
        "          '''\n",
        "          squared_gradients={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "              squared_gradients[f'w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "              squared_gradients[f'b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          for i in range(self.epochs):\n",
        "              for j in range(0,self.no_of_samples,self.batch_size):\n",
        "                  start = j\n",
        "                  end = start + self.batch_size\n",
        "                  Y_train_labels = self.train_labels[start:end]\n",
        "                  X_train_images = self.train_data[start:end]\n",
        "\n",
        "                  pred_labels=self.feed_forwards(X_train_images)\n",
        "                  self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "\n",
        "                  for k in range(len(self.layer_sizes) - 1):\n",
        "\n",
        "                      self.gradients[f'w_{k+1}']=self.gradients[f'w_{k+1}']/self.batch_size\n",
        "                      self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                      self.gradients[f'w_{k+1}'] =(self.gradients[f'w_{k+1}']) + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "\n",
        "                      # Calculate the squared gradients\n",
        "                      squared_gradients[f'w_{k+1}'] = self.beta * squared_gradients[f'w_{k+1}'] + (1 - self.beta) * ((self.gradients[f'w_{k+1}']) ** 2)\n",
        "                      squared_gradients[f'b_{k+1}'] = self.beta * squared_gradients[f'b_{k+1}'] + (1 - self.beta) * ((self.gradients[f'b_{k+1}']) ** 2)\n",
        "\n",
        "                      # Update parameters using RMSprop\n",
        "                      self.parameters[f'w_{k+1}'] -= (self.learning_rate * self.gradients[f'w_{k+1}'] / (np.sqrt(squared_gradients[f'w_{k+1}']) + self.epsilon))\n",
        "                      self.parameters[f'b_{k+1}'] -= (self.learning_rate * self.gradients[f'b_{k+1}'] / (np.sqrt(squared_gradients[f'b_{k+1}']) + self.epsilon))\n",
        "\n",
        "              pred_labels = self.feed_forwards(self.train_data)\n",
        "              tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "              tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "              pred_labels = self.feed_forwards(self.val_data)\n",
        "              val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "              val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "              print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "              wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "        if(self.optimizer==\"adam\"):\n",
        "          '''\n",
        "          Epoch 10, Loss: 0.6223362734184407 Acc: 77.985\n",
        "          TEST LOSS: 0.6613 ACCURACY: 76.2700\n",
        "          '''\n",
        "          moments={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "              moments[f'm_w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "              moments[f'm_b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          velocities={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "              velocities[f'v_w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "              velocities[f'v_b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          # t=0\n",
        "          for i in range(self.epochs):\n",
        "            t=i+1\n",
        "            for j in range(0,self.no_of_samples,self.batch_size):\n",
        "                # t=t+1\n",
        "                start = j\n",
        "                end = start + self.batch_size\n",
        "                Y_train_labels = self.train_labels[start:end]\n",
        "                X_train_images = self.train_data[start:end]\n",
        "\n",
        "                pred_labels=self.feed_forwards(X_train_images)\n",
        "                self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "                # t=1\n",
        "                for k in range(len(self.layer_sizes) - 1):\n",
        "\n",
        "                    self.gradients[f'w_{k+1}'] = self.gradients[f'w_{k+1}'] / self.batch_size\n",
        "                    self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                    self.gradients[f'w_{k+1}'] = self.gradients[f'w_{k+1}'] + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "\n",
        "                    # Update moments and velocities with Adam\n",
        "                    moments[f'm_w_{k+1}'] = self.beta1 * moments[f'm_w_{k+1}'] + (1 - self.beta1) * self.gradients[f'w_{k+1}']\n",
        "                    moments[f'm_b_{k+1}'] = self.beta1 * moments[f'm_b_{k+1}'] + (1 - self.beta1) * self.gradients[f'b_{k+1}']\n",
        "\n",
        "                    velocities[f'v_w_{k+1}'] = self.beta2 * velocities[f'v_w_{k+1}'] + (1 - self.beta2) * (self.gradients[f'w_{k+1}'] ** 2)\n",
        "                    velocities[f'v_b_{k+1}'] = self.beta2 * velocities[f'v_b_{k+1}'] + (1 - self.beta2) * (self.gradients[f'b_{k+1}'] ** 2)\n",
        "\n",
        "                    # Bias correction\n",
        "                    m_w_hat = moments[f'm_w_{k+1}'] / (1 - self.beta1 ** t)\n",
        "                    m_b_hat = moments[f'm_b_{k+1}'] / (1 - self.beta1 ** t)\n",
        "\n",
        "                    v_w_hat = velocities[f'v_w_{k+1}'] / (1 - self.beta2 ** t)\n",
        "                    v_b_hat = velocities[f'v_b_{k+1}'] / (1 - self.beta2 ** t)\n",
        "\n",
        "                    # Update parameters with Adam\n",
        "                    self.parameters[f'w_{k+1}'] -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
        "                    self.parameters[f'b_{k+1}'] -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
        "\n",
        "                    # t=t+1\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.train_data)\n",
        "            tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "            tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.val_data)\n",
        "            val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "            val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "            print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "            wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "        if(self.optimizer==\"nadam\"):\n",
        "          '''\n",
        "          Epoch 10, Loss: 0.645183860082157 Acc: 78.36333333333333\n",
        "          TEST LOSS: 0.6794 ACCURACY: 77.5300\n",
        "          '''\n",
        "          moments={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "              moments[f'm_w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "              moments[f'm_b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          velocities={}\n",
        "          for i in range(len(self.layer_sizes) - 1):\n",
        "              velocities[f'v_w_{i+1}'] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "              velocities[f'v_b_{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "          # t=0\n",
        "          for i in range(self.epochs):\n",
        "            t=1+i\n",
        "            for j in range(0,self.no_of_samples,self.batch_size):\n",
        "              # t=t+1\n",
        "              start = j\n",
        "              end = start + self.batch_size\n",
        "              Y_train_labels = self.train_labels[start:end]\n",
        "              X_train_images = self.train_data[start:end]\n",
        "\n",
        "              pred_labels=self.feed_forwards(X_train_images)\n",
        "              self.back_prop(X_train_images, Y_train_labels,pred_labels)\n",
        "\n",
        "              for k in range(len(self.layer_sizes) - 1):\n",
        "\n",
        "                  self.gradients[f'w_{k+1}'] = self.gradients[f'w_{k+1}'] / self.batch_size\n",
        "                  self.gradients[f'b_{k+1}']=self.gradients[f'b_{k+1}']/self.batch_size\n",
        "\n",
        "                  self.gradients[f'w_{k+1}'] = self.gradients[f'w_{k+1}'] + self.weight_decay * self.parameters[f'w_{k+1}']\n",
        "\n",
        "                  # Update moments and velocities with Adam\n",
        "                  moments[f'm_w_{k+1}'] = self.beta1 * moments[f'm_w_{k+1}'] + (1 - self.beta1) * self.gradients[f'w_{k+1}']\n",
        "                  moments[f'm_b_{k+1}'] = self.beta1 * moments[f'm_b_{k+1}'] + (1 - self.beta1) * self.gradients[f'b_{k+1}']\n",
        "\n",
        "                  velocities[f'v_w_{k+1}'] = self.beta2 * velocities[f'v_w_{k+1}'] + (1 - self.beta2) * (self.gradients[f'w_{k+1}'] ** 2)\n",
        "                  velocities[f'v_b_{k+1}'] = self.beta2 * velocities[f'v_b_{k+1}'] + (1 - self.beta2) * (self.gradients[f'b_{k+1}'] ** 2)\n",
        "\n",
        "                  # Bias correction\n",
        "                  m_w_hat = moments[f'm_w_{k+1}'] / (1 - self.beta1 ** t)\n",
        "                  m_b_hat = moments[f'm_b_{k+1}'] / (1 - self.beta1 ** t)\n",
        "\n",
        "                  v_w_hat = velocities[f'v_w_{k+1}'] / (1 - self.beta2 ** t)\n",
        "                  v_b_hat = velocities[f'v_b_{k+1}'] / (1 - self.beta2 ** t)\n",
        "\n",
        "                  # Update parameters with NAdam\n",
        "                  nadam_factor = (1 - self.beta1) / (1 - (self.beta1 ** t))\n",
        "                  weigth_update = nadam_factor * self.gradients[f'w_{k+1}'] + (self.beta1 * m_w_hat)\n",
        "                  bias_update = nadam_factor * self.gradients[f'b_{k+1}'] + (self.beta1 * m_b_hat)\n",
        "                  self.parameters[f'w_{k+1}'] -= ((self.learning_rate / (np.sqrt(v_w_hat) + self.epsilon)) * weigth_update )\n",
        "                  self.parameters[f'b_{k+1}'] -= ((self.learning_rate / (np.sqrt(v_b_hat) + self.epsilon)) * bias_update )\n",
        "\n",
        "              # t=t+1\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.train_data)\n",
        "            tra_acc = self.accuracy(pred_labels,self.train_labels)\n",
        "            tra_loss = self.loss_act_func.loss_functions(pred_labels, self.train_labels)\n",
        "\n",
        "            pred_labels = self.feed_forwards(self.val_data)\n",
        "            val_acc = self.accuracy(pred_labels,self.val_labels)\n",
        "            val_loss = self.loss_act_func.loss_functions(pred_labels, self.val_labels)\n",
        "            print(f\"Epoch {i+1}, Training Loss: {tra_loss} , Training Accuracy: {tra_acc} , Validation Loss: {val_loss} Validation Accuracy: {val_acc}\")\n",
        "            wandb.log({\"Epoch\": i+1, \"Training_Loss\": tra_loss, \"Training_Accuracy\": tra_acc,\"Validation_Loss\": val_loss,\"Validation_Accuracy\": val_acc})\n",
        "\n",
        "    def accuracy(self, pred, truth):\n",
        "      return ((np.argmax(truth, axis=1) == np.argmax(pred, axis=1)).mean())*100\n",
        "\n",
        "    def test(self, x, y):\n",
        "      pred_labels=self.feed_forwards(x)\n",
        "      loss = self.loss_act_func.loss_functions(pred_labels, y)\n",
        "      acc = self.accuracy(pred_labels, y)\n",
        "      print(f'TEST LOSS: {loss:.4f} ACCURACY: {acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# loss and activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LossActFunc():\n",
        "    def __init__(self, lossFun = \"cross_entropy\",actFun=\"sigmoid\"):\n",
        "        self.loss_func = lossFun\n",
        "        self.act_fun = actFun\n",
        "\n",
        "    def loss_functions(self,pred, actual):\n",
        "      if (self.loss_func==\"cross_entropy\"):\n",
        "        return -np.sum(actual * np.log(pred + 1e-9)) / len(pred)\n",
        "\n",
        "      elif (self.loss_func == \"mean_squared_error\"):\n",
        "        squErr=(1/2)* np.sum((pred - actual) ** 2)\n",
        "        meanSquErr=squErr/ len(pred)\n",
        "        return  meanSquErr\n",
        "\n",
        "    def get_name(self):\n",
        "      if (self.loss_func==\"cross_entropy\"):\n",
        "        return \"cross_entropy\"\n",
        "\n",
        "      elif (self.loss_func == \"mean_squared_error\"):\n",
        "        return \"mean_squared_error\"\n",
        "\n",
        "\n",
        "    def activation_func(self, x):\n",
        "\n",
        "      if(self.act_fun == \"sigmoid\"):\n",
        "        # sigmoid function\n",
        "        # print(\"in sigmoid\",x)\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "      elif(self.act_fun == \"tanh\"):\n",
        "          # tanh function\n",
        "          return np.tanh(x)\n",
        "\n",
        "      elif(self.act_fun == \"ReLU\"):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def derivation_of_activation(self, x):\n",
        "\n",
        "      if(self.act_fun == \"sigmoid\"):\n",
        "        sigmoid_x = self.activation_func(x)\n",
        "        return sigmoid_x * (1 - sigmoid_x)\n",
        "\n",
        "      if(self.act_fun == \"tanh\"):\n",
        "          return 1 - (np.tanh(x) ** 2)\n",
        "\n",
        "      if(self.act_fun == \"ReLU\"):\n",
        "        return (x > 0) * 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class ActivationFunctions():\n",
        "#     def __init__(self, fun = \"sigmoid\"):\n",
        "#         self.act_fun = fun\n",
        "\n",
        "#     def activation_func(self, x):\n",
        "\n",
        "#         if(self.act_fun == \"sigmoid\"):\n",
        "#           # sigmoid function\n",
        "#           # print(\"in sigmoid\",x)\n",
        "#           return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "#         elif(self.act_fun == \"tanh\"):\n",
        "#             # tanh function\n",
        "#             return np.tanh(x)\n",
        "\n",
        "#         elif(self.act_fun == \"ReLU\"):\n",
        "#           return np.maximum(0, x)\n",
        "\n",
        "#     def derivation_of_activation(self, x):\n",
        "\n",
        "#         if(self.act_fun == \"sigmoid\"):\n",
        "#           sigmoid_x = self.activation_func(x)\n",
        "#           return sigmoid_x * (1 - sigmoid_x)\n",
        "\n",
        "#         if(self.act_fun == \"tanh\"):\n",
        "#             return 1 - (np.tanh(x) ** 2)\n",
        "\n",
        "#         if(self.act_fun == \"ReLU\"):\n",
        "#           return (x > 0) * 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load & Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "total_class=max(train_labels)\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], -1) / 255.0\n",
        "test_images = test_images.reshape(test_images.shape[0], -1) / 255.0\n",
        "\n",
        "#change trainlabel=2 to oneHot=[0 0 1 0 0 0 0 0 0 0]\n",
        "train_labels_one_hot = np.eye(10)[train_labels]\n",
        "test_labels_one_hot = np.eye(10)[test_labels]\n",
        "\n",
        "train_images, val_images, train_labels_one_hot, val_labels_one_hot = train_test_split(train_images, train_labels_one_hot, test_size=0.1, random_state=42)\n",
        "# print(len(train_images))\n",
        "# print(len(val_images))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set param and call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer_sizes = [train_images.shape[1],256,512,10]  # Input, hidden1, hidden2,hidden3 output\n",
        "\n",
        "# gent Starting Run: dlaobhoi with config:\n",
        "# wandb: \tactivation: ReLU\n",
        "# wandb: \tbatch_size: 64\n",
        "# wandb: \tepochs: 5\n",
        "# wandb: \thidden_layers: [128, 128, 128, 128, 128]\n",
        "# wandb: \tinitialization: random\n",
        "# wandb: \tlearning_rate: 0.001\n",
        "# wandb: \toptimizer: momentum\n",
        "# wandb: \tweight_decay: 0.5\n",
        "\n",
        "loss_act_func = LossActFunc(\"cross_entropy\",\"ReLU\") #\"mean_squared_error\", \"cross_entropy\" # sigmoid, ReLU, tanh\n",
        "# loss_func = LossFunc(\"cross_entropy\")\n",
        "epochs=5\n",
        "initialization=\"Xavier\" #random , Xavier\n",
        "learning_rate=0.0005\n",
        "momentum=0.9  #for momentum and nag\n",
        "beta=0.5     #for rmsprop\n",
        "beta1=0.9    #for adam and nadam\n",
        "beta2=0.999    #for adam and nadam\n",
        "epsilon=1e-8\n",
        "weight_decay=0\n",
        "batch_size=32\n",
        "optimizer=\"nadam\" # sgd , momentum ,nag , rmsprop , adam , nadam\n",
        "\n",
        "# Epoch 50, Training Loss: 0.09106023231455047 , Training Accuracy: 96.66481481481482 , Validation Loss: 0.5375967565066632 Validation Accuracy: 88.78333333333333\n",
        "# TEST LOSS: 0.5613 ACCURACY: 88.5200\n",
        "\n",
        "mlp = MultiLayerPerceptron(train_images, train_labels_one_hot,val_images,val_labels_one_hot, layer_sizes,initialization,loss_act_func,batch_size,epochs,learning_rate,weight_decay,optimizer,momentum,beta,beta1,beta2,epsilon)\n",
        "mlp.init_weights()\n",
        "mlp.optimizer_func()\n",
        "mlp.test(test_images, test_labels_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# question4 sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    # RandomSearch over the hyperparameters\n",
        "    \"method\": \"random\",\n",
        "    \"name\":\"Random_sweep\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"validationaccuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLU\"]},\n",
        "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        \"no_of_hidden_layer\": {\"values\": [3,4,5]},\n",
        "        \"size_of_hidden_layerr\": {\"values\": [256,128,64,32]},\n",
        "        \"epochs\": {\"values\": [5, 10, 30]},\n",
        "        \"weight_decay\": {\"values\": [0.0, 0.0005, 0.5]},\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"initialization\": {\"values\": [\"Xavier\", \"random\"]}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "   init_sweep =  wandb.init(project=\"CS6910 - Assignment 1\", name=\"Question4\")\n",
        "   sweep_params = init_sweep.config\n",
        "\n",
        "   wandb.run.name = \"_ac_\" + sweep_params.activation + \"_bs_\" + str(sweep_params.batch_size) + \"_op_\" + sweep_params.optimizer + \"_hl_\" + str(sweep_params.no_of_hidden_layer)+ \"_neu_\" + str(sweep_params.size_of_hidden_layerr) +\"_ep_\" + str(sweep_params.epochs)+\"_wd_\" + str(sweep_params.weight_decay)+\"_lr_\" + str(sweep_params.learning_rate)+\"_ini_\"+sweep_params.initialization\n",
        "\n",
        "   loss_act_func = LossActFunc(\"cross_entropy\",\"ReLU\")\n",
        "   epochs=sweep_params.epochs\n",
        "   initialization=sweep_params.initialization #random , Xavier\n",
        "   learning_rate=sweep_params.learning_rate\n",
        "   momentum=0.9  #for momentum and nag\n",
        "   beta=0.9     #for rmsprop\n",
        "   beta1=0.9    #for adam and nadam\n",
        "   beta2=0.999    #for adam and nadam\n",
        "   epsilon=0.000001\n",
        "   weight_decay=sweep_params.weight_decay\n",
        "   batch_size=sweep_params.batch_size\n",
        "   optimizer=sweep_params.optimizer # sgd , momentum ,nag , rmsprop , adam , nadam\n",
        "\n",
        "   # layer_sizes = [train_images.shape[1],5,10,15,10]\n",
        "   layer_sizes=[]\n",
        "   layer_sizes.append(train_images.shape[1])\n",
        "   for i in range(sweep_params.no_of_hidden_layer):\n",
        "      layer_sizes.append(sweep_params.size_of_hidden_layerr)\n",
        "   layer_sizes.append(10)\n",
        "\n",
        "   print(\"layer_sizes=\",layer_sizes)\n",
        "\n",
        "   mlp = MultiLayerPerceptron(train_images, train_labels_one_hot,val_images,val_labels_one_hot, layer_sizes,initialization,loss_act_func,batch_size,epochs,learning_rate,weight_decay,optimizer,momentum,beta,beta1,beta2,epsilon)\n",
        "   mlp.init_weights()\n",
        "   mlp.optimizer_func()\n",
        "   y_pred = mlp.feed_forwards(val_images)\n",
        "\n",
        "   val_accuracy=mlp.accuracy(y_pred,val_labels_one_hot)\n",
        "   wandb.log({\"validationaccuracy\": val_accuracy})\n",
        "   # mlp.test(test_images, test_labels_one_hot)\n",
        "   #  print(\"test_acc=\",test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 - Assignment 1\")\n",
        "wandb.agent(sweep_id, train, count=100)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q4_bayesian_sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    # RandomSearch over the hyperparameters\n",
        "    \"name\" : \"Bayesian Sweep\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"validationaccuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLU\"]},\n",
        "        \"batch_size\": {\"values\": [32, 64, 128]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        \"no_of_hidden_layer\": {\"values\": [3,4,5]},\n",
        "        \"size_of_hidden_layerr\": {\"values\": [256,128,64,32]},\n",
        "        \"epochs\": {\"values\": [5, 10, 30]},\n",
        "        \"weight_decay\": {\"values\": [0.0, 0.0005, 0.5]},\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"initialization\": {\"values\": [\"Xavier\", \"random\"]}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "   init_sweep =  wandb.init(project=\"CS6910 - Assignment 1\", name=\"Question4_bayes\")\n",
        "   sweep_params = init_sweep.config\n",
        "\n",
        "   wandb.run.name = \"_ac_\" + sweep_params.activation + \"_bs_\" + str(sweep_params.batch_size) + \"_op_\" + sweep_params.optimizer +\"_hl_\" + str(sweep_params.no_of_hidden_layer)+ \"_neu_\" + str(sweep_params.size_of_hidden_layerr) +\"_ep_\" + str(sweep_params.epochs)+\"_wd_\" + str(sweep_params.weight_decay)+\"_lr_\" + str(sweep_params.learning_rate)+\"_ini_\"+sweep_params.initialization\n",
        "\n",
        "   loss_act_func = LossActFunc(\"cross_entropy\",\"ReLU\")\n",
        "    # sigmoid, ReLU, tanh\n",
        "    #\"mean_squared_error\", \"cross_entropy\"\n",
        "   epochs=sweep_params.epochs\n",
        "   initialization=sweep_params.initialization #random , Xavier\n",
        "   learning_rate=sweep_params.learning_rate\n",
        "   momentum=0.9  #for momentum and nag\n",
        "   beta=0.9     #for rmsprop\n",
        "   beta1=0.9    #for adam and nadam\n",
        "   beta2=0.999    #for adam and nadam\n",
        "   epsilon=0.000001\n",
        "   weight_decay=sweep_params.weight_decay\n",
        "   batch_size=sweep_params.batch_size\n",
        "   optimizer=sweep_params.optimizer # sgd , momentum ,nag , rmsprop , adam , nadam\n",
        "\n",
        "   # layer_sizes = [train_images.shape[1],5,10,15,10]\n",
        "   layer_sizes=[]\n",
        "   layer_sizes.append(train_images.shape[1])\n",
        "   for i in range(sweep_params.no_of_hidden_layer):\n",
        "      layer_sizes.append(sweep_params.size_of_hidden_layerr)\n",
        "   layer_sizes.append(10)\n",
        "\n",
        "   print(\"layer_sizes=\",layer_sizes)\n",
        "\n",
        "   mlp = MultiLayerPerceptron(train_images, train_labels_one_hot,val_images,val_labels_one_hot, layer_sizes,initialization,loss_act_func,batch_size,epochs,learning_rate,weight_decay,optimizer,momentum,beta,beta1,beta2,epsilon)\n",
        "   mlp.init_weights()\n",
        "   mlp.optimizer_func()\n",
        "   y_pred = mlp.feed_forwards(val_images)\n",
        "\n",
        "   val_accuracy=mlp.accuracy(y_pred,val_labels_one_hot)\n",
        "   wandb.log({\"validationaccuracy\": val_accuracy})\n",
        "   # mlp.test(test_images, test_labels_one_hot)\n",
        "   #  print(\"test_acc=\",test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 - Assignment 1\")\n",
        "wandb.agent(sweep_id, train, count=100)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# question7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#set best params\n",
        "layer_sizes = [train_images.shape[1],256,256,256,128,128,10]  # Input, hidden1, hidden2,hidden3 output\n",
        "\n",
        "act_func = ActivationFunctions(\"tanh\") # sigmoid, ReLU, tanh\n",
        "loss_func = LossFunc(\"cross_entropy\") #\"mean_squared_error\", \"cross_entropy\"\n",
        "epochs=30\n",
        "initialization=\"Xavier\" #random , Xavier\n",
        "learning_rate=0.0001\n",
        "momentum=0.9  #for momentum and nag\n",
        "beta=0.5     #for rmsprop\n",
        "beta1=0.9    #for adam and nadam\n",
        "beta2=0.999    #for adam and nadam\n",
        "epsilon=1e-8\n",
        "weight_decay=0\n",
        "batch_size=32\n",
        "optimizer=\"adam\" # sgd , momentum ,nag , rmsprop , adam , nadam\n",
        "\n",
        "# Epoch 50, Training Loss: 0.09106023231455047 , Training Accuracy: 96.66481481481482 , Validation Loss: 0.5375967565066632 Validation Accuracy: 88.78333333333333\n",
        "# TEST LOSS: 0.5613 ACCURACY: 88.5200\n",
        "\n",
        "mlp = MultiLayerPerceptron(train_images, train_labels_one_hot,val_images,val_labels_one_hot, layer_sizes,initialization,act_func,loss_func,batch_size,epochs,learning_rate,weight_decay,optimizer,momentum,beta,beta1,beta2,epsilon)\n",
        "mlp.init_weights()\n",
        "mlp.optimizer_func()\n",
        "mlp.test(test_images, test_labels_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "className_images = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "pred_class = mlp.feed_forwards(test_images)\n",
        "\n",
        "wandb.init(project=\"CS6910 - Assignment 1\", name=\"Question7\")\n",
        "\n",
        "# # print(len(pred_class))\n",
        "# print(np.argmax(pred_class,1)[0])\n",
        "# print(np.argmax(pred_class[0]))\n",
        "\n",
        "mat_predicted=[]\n",
        "mat_true=[]\n",
        "for i in range(len(pred_class)):\n",
        "  mat_predicted.append(np.argmax(pred_class[i]))\n",
        "for i in range(len(pred_class)):\n",
        "  mat_true.append(np.argmax(test_labels_one_hot[i]))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "confusion_matrix = metrics.confusion_matrix(mat_true , mat_predicted)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = className_images)\n",
        "plt.title('Confusion Matrix Fashion', fontsize=10)\n",
        "cm_display.plot(ax=ax)\n",
        "# plt.show()\n",
        "\n",
        "wandb.log({\"Confusion_Matrix\": wandb.Image(plt)})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "128e704b5b4e42dea9ed2019a2fd7fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38120d2c360d4183b486f9dde5077042": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1199cacbab47c6a1b34884c8f49b3e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b2bb6d2d4fc4244a63b007f7d7f58a3",
            "value": 1
          }
        },
        "422951a3b0714d37baa4fb0df1b9ce69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546924928b084f1f8f184f4d268c84b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1199cacbab47c6a1b34884c8f49b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2bb6d2d4fc4244a63b007f7d7f58a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e04090ea27f94de7a864ec19bcc24a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f50afc3441694398a5b2a0b2a77dbb5a",
              "IPY_MODEL_38120d2c360d4183b486f9dde5077042"
            ],
            "layout": "IPY_MODEL_422951a3b0714d37baa4fb0df1b9ce69"
          }
        },
        "f50afc3441694398a5b2a0b2a77dbb5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_546924928b084f1f8f184f4d268c84b6",
            "placeholder": "​",
            "style": "IPY_MODEL_128e704b5b4e42dea9ed2019a2fd7fe2",
            "value": "0.016 MB of 0.016 MB uploaded\r"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
